{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import youtube_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytube import YouTube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path='../data/videos/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YouTube('https://youtu.be/9bZkp7q19f0').streams.first().download()\n",
    "yt = YouTube('http://youtube.com/watch?v=9bZkp7q19f0')\n",
    "stream=yt.streams.filter(progressive=True, file_extension='mp4')\\\n",
    ".order_by('resolution')\\\n",
    ".desc()\\\n",
    ".first()\n",
    "\n",
    "# st.download(output_path=output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream.default_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.rename('../data/videos/Gangnam Style.mp4','../data/videos/abc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video file path\n",
    "video_path = '../data/videos/Gangnam Style.mp4'\n",
    "\n",
    "# Output folder path\n",
    "output_folder = '../data/images_1_sec/abc'\n",
    "\n",
    "if os.path.exists(output_folder) == False:\n",
    "    os.mkdir(output_folder)\n",
    "\n",
    "# Open the video file\n",
    "video = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get the frames per second (fps) of the video\n",
    "fps = video.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Set the desired interval in seconds\n",
    "interval = 1\n",
    "\n",
    "# Calculate the frame interval based on the fps\n",
    "frame_interval = math.ceil(fps * interval)\n",
    "\n",
    "# Read and save frames at the specified interval\n",
    "frame_count = 0\n",
    "while True:\n",
    "    # Read the next frame\n",
    "    success, frame = video.read()\n",
    "\n",
    "    # Check if the frame was read successfully\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Save the frame as an image\n",
    "    if frame_count % frame_interval == 0:\n",
    "        image_path = f\"{output_folder}/frame_{frame_count}.jpg\"\n",
    "        cv2.imwrite(image_path, frame)\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "# Release the video capture object\n",
    "video.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gangnam Style.mp4'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream.default_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HADOOP_CONF_DIR']='/data/DSTeam/DScommon/hadoop-3.3.6/'\n",
    "os.environ['JAVA_HOME']='/usr/lib/jvm/java-11-openjdk-amd64'\n",
    "os.environ['HADOOP_USER_NAME']='locnt1'\n",
    "os.environ['PYTHONPAH']='../.'\n",
    "os.environ['PATH']=os.environ['PATH']+\":../.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/DSTeam/locnt1/personal/thesis/env/bin:/home/locnt1/.vscode-server/bin/74b1f979648cc44d385a2286793c226e611f59e7/bin/remote-cli:/data/DSTeam/DScommon/anaconda3/bin:/data/DSTeam/DScommon/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:../.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.fs as fs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3495300/997502432.py:1: FutureWarning: pyarrow.hdfs.connect is deprecated as of 2.0.0, please use pyarrow.fs.HadoopFileSystem instead.\n",
      "  fs_connector = pa.hdfs.connect(host='default', port=0,user='locnt1')\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'hadoop'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fs_connector \u001b[38;5;241m=\u001b[39m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhdfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdefault\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43muser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlocnt1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/DSTeam/locnt1/personal/thesis/env/lib/python3.8/site-packages/pyarrow/hdfs.py:227\u001b[0m, in \u001b[0;36mconnect\u001b[0;34m(host, port, user, kerb_ticket, extra_conf)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[39mDEPRECATED: Connect to an HDFS cluster.\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[39mfilesystem : HadoopFileSystem\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    223\u001b[0m warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    224\u001b[0m     _DEPR_MSG\u001b[39m.\u001b[39mformat(\u001b[39m\"\u001b[39m\u001b[39mhdfs.connect\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m2.0.0\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfs.HadoopFileSystem\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m    225\u001b[0m     \u001b[39mFutureWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m\n\u001b[1;32m    226\u001b[0m )\n\u001b[0;32m--> 227\u001b[0m \u001b[39mreturn\u001b[39;00m _connect(\n\u001b[1;32m    228\u001b[0m     host\u001b[39m=\u001b[39;49mhost, port\u001b[39m=\u001b[39;49mport, user\u001b[39m=\u001b[39;49muser, kerb_ticket\u001b[39m=\u001b[39;49mkerb_ticket,\n\u001b[1;32m    229\u001b[0m     extra_conf\u001b[39m=\u001b[39;49mextra_conf\n\u001b[1;32m    230\u001b[0m )\n",
      "File \u001b[0;32m/data/DSTeam/locnt1/personal/thesis/env/lib/python3.8/site-packages/pyarrow/hdfs.py:237\u001b[0m, in \u001b[0;36m_connect\u001b[0;34m(host, port, user, kerb_ticket, extra_conf)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[39mwith\u001b[39;00m warnings\u001b[39m.\u001b[39mcatch_warnings():\n\u001b[1;32m    236\u001b[0m     warnings\u001b[39m.\u001b[39msimplefilter(\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 237\u001b[0m     fs \u001b[39m=\u001b[39m HadoopFileSystem(host\u001b[39m=\u001b[39;49mhost, port\u001b[39m=\u001b[39;49mport, user\u001b[39m=\u001b[39;49muser,\n\u001b[1;32m    238\u001b[0m                           kerb_ticket\u001b[39m=\u001b[39;49mkerb_ticket,\n\u001b[1;32m    239\u001b[0m                           extra_conf\u001b[39m=\u001b[39;49mextra_conf)\n\u001b[1;32m    240\u001b[0m \u001b[39mreturn\u001b[39;00m fs\n",
      "File \u001b[0;32m/data/DSTeam/locnt1/personal/thesis/env/lib/python3.8/site-packages/pyarrow/hdfs.py:47\u001b[0m, in \u001b[0;36mHadoopFileSystem.__init__\u001b[0;34m(self, host, port, user, kerb_ticket, driver, extra_conf)\u001b[0m\n\u001b[1;32m     42\u001b[0m warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m     43\u001b[0m     _DEPR_MSG\u001b[39m.\u001b[39mformat(\n\u001b[1;32m     44\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhdfs.HadoopFileSystem\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m2.0.0\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfs.HadoopFileSystem\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m     45\u001b[0m     \u001b[39mFutureWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[39mif\u001b[39;00m driver \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mlibhdfs\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m---> 47\u001b[0m     _maybe_set_hadoop_classpath()\n\u001b[1;32m     49\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connect(host, port, user, kerb_ticket, extra_conf)\n",
      "File \u001b[0;32m/data/DSTeam/locnt1/personal/thesis/env/lib/python3.8/site-packages/pyarrow/hdfs.py:147\u001b[0m, in \u001b[0;36m_maybe_set_hadoop_classpath\u001b[0;34m()\u001b[0m\n\u001b[1;32m    145\u001b[0m         classpath \u001b[39m=\u001b[39m _hadoop_classpath_glob(hadoop_bin)\n\u001b[1;32m    146\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 147\u001b[0m     classpath \u001b[39m=\u001b[39m _hadoop_classpath_glob(\u001b[39m'\u001b[39;49m\u001b[39mhadoop\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    149\u001b[0m os\u001b[39m.\u001b[39menviron[\u001b[39m'\u001b[39m\u001b[39mCLASSPATH\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m classpath\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/data/DSTeam/locnt1/personal/thesis/env/lib/python3.8/site-packages/pyarrow/hdfs.py:172\u001b[0m, in \u001b[0;36m_hadoop_classpath_glob\u001b[0;34m(hadoop_bin)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msubprocess\u001b[39;00m\n\u001b[1;32m    171\u001b[0m hadoop_classpath_args \u001b[39m=\u001b[39m (hadoop_bin, \u001b[39m'\u001b[39m\u001b[39mclasspath\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m--glob\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[39mreturn\u001b[39;00m subprocess\u001b[39m.\u001b[39;49mcheck_output(hadoop_classpath_args)\n",
      "File \u001b[0;32m/usr/lib/python3.8/subprocess.py:415\u001b[0m, in \u001b[0;36mcheck_output\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    412\u001b[0m         empty \u001b[39m=\u001b[39m \u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    413\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39minput\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m empty\n\u001b[0;32m--> 415\u001b[0m \u001b[39mreturn\u001b[39;00m run(\u001b[39m*\u001b[39;49mpopenargs, stdout\u001b[39m=\u001b[39;49mPIPE, timeout\u001b[39m=\u001b[39;49mtimeout, check\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    416\u001b[0m            \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\u001b[39m.\u001b[39mstdout\n",
      "File \u001b[0;32m/usr/lib/python3.8/subprocess.py:493\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    490\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mstdout\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m PIPE\n\u001b[1;32m    491\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mstderr\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m PIPE\n\u001b[0;32m--> 493\u001b[0m \u001b[39mwith\u001b[39;00m Popen(\u001b[39m*\u001b[39;49mpopenargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs) \u001b[39mas\u001b[39;00m process:\n\u001b[1;32m    494\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    495\u001b[0m         stdout, stderr \u001b[39m=\u001b[39m process\u001b[39m.\u001b[39mcommunicate(\u001b[39minput\u001b[39m, timeout\u001b[39m=\u001b[39mtimeout)\n",
      "File \u001b[0;32m/usr/lib/python3.8/subprocess.py:858\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_mode:\n\u001b[1;32m    855\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mTextIOWrapper(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr,\n\u001b[1;32m    856\u001b[0m                     encoding\u001b[39m=\u001b[39mencoding, errors\u001b[39m=\u001b[39merrors)\n\u001b[0;32m--> 858\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_child(args, executable, preexec_fn, close_fds,\n\u001b[1;32m    859\u001b[0m                         pass_fds, cwd, env,\n\u001b[1;32m    860\u001b[0m                         startupinfo, creationflags, shell,\n\u001b[1;32m    861\u001b[0m                         p2cread, p2cwrite,\n\u001b[1;32m    862\u001b[0m                         c2pread, c2pwrite,\n\u001b[1;32m    863\u001b[0m                         errread, errwrite,\n\u001b[1;32m    864\u001b[0m                         restore_signals, start_new_session)\n\u001b[1;32m    865\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m    866\u001b[0m     \u001b[39m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[1;32m    867\u001b[0m     \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m \u001b[39mfilter\u001b[39m(\u001b[39mNone\u001b[39;00m, (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstdin, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstdout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr)):\n",
      "File \u001b[0;32m/usr/lib/python3.8/subprocess.py:1704\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1702\u001b[0m     \u001b[39mif\u001b[39;00m errno_num \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1703\u001b[0m         err_msg \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mstrerror(errno_num)\n\u001b[0;32m-> 1704\u001b[0m     \u001b[39mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1705\u001b[0m \u001b[39mraise\u001b[39;00m child_exception_type(err_msg)\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'hadoop'"
     ]
    }
   ],
   "source": [
    "fs_connector = pa.hdfs.connect(host='default', port=0,user='locnt1')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_video(\n",
    "    video_url:str\n",
    "):\n",
    "    yt = YouTube(video_url)\n",
    "    yt.streams.filter(progressive=True, file_extension='mp4')\\\n",
    "    .order_by('resolution')\\\n",
    "    .desc()\\\n",
    "    .first()\\\n",
    "    .download()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.video_preprocessor import video_handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_handler.download_video(\n",
    "    video_id=\"7JJfJgyHYwU\", \n",
    "    output_dir = \"../data/videos\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_handler.split_video_to_images(\n",
    "    video_path='/data/DSTeam/locnt1/personal/thesis/data/videos/7JJfJgyHYwU', output_images_directory='/data/DSTeam/locnt1/personal/thesis/data/images/7JJfJgyHYwU'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/07 09:47:18 WARN Utils: Your hostname, sysadmin resolves to a loopback address: 127.0.1.1; using 10.23.5.36 instead (on interface bond-external)\n",
      "23/09/07 09:47:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/data/DSTeam/DScommon/spark-3.2.3-bin-hadoop2.7/jars/spark-unsafe_2.12-3.2.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/07 09:47:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/09/07 09:47:18 WARN Utils: Service 'sparkDriver' could not bind on port 40000. Attempting port 40001.\n",
      "23/09/07 09:47:18 WARN Utils: Service 'sparkDriver' could not bind on port 40001. Attempting port 40002.\n",
      "23/09/07 09:47:18 WARN Utils: Service 'sparkDriver' could not bind on port 40002. Attempting port 40003.\n",
      "23/09/07 09:47:18 WARN Utils: Service 'sparkDriver' could not bind on port 40003. Attempting port 40004.\n",
      "23/09/07 09:47:18 WARN Utils: Service 'sparkDriver' could not bind on port 40004. Attempting port 40005.\n",
      "23/09/07 09:47:18 WARN Utils: Service 'sparkDriver' could not bind on port 40005. Attempting port 40006.\n",
      "23/09/07 09:47:18 WARN Utils: Service 'sparkDriver' could not bind on port 40006. Attempting port 40007.\n",
      "23/09/07 09:47:19 WARN Utils: Service 'sparkDriver' could not bind on port 40007. Attempting port 40008.\n",
      "23/09/07 09:47:19 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 40000. Attempting port 40001.\n",
      "23/09/07 09:47:19 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 40001. Attempting port 40002.\n",
      "23/09/07 09:47:19 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 40002. Attempting port 40003.\n",
      "23/09/07 09:47:19 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 40003. Attempting port 40004.\n",
      "23/09/07 09:47:19 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 40004. Attempting port 40005.\n",
      "23/09/07 09:47:19 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 40005. Attempting port 40006.\n",
      "23/09/07 09:47:19 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 40006. Attempting port 40007.\n",
      "23/09/07 09:47:19 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 40007. Attempting port 40008.\n",
      "23/09/07 09:47:19 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 40008. Attempting port 40009.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# import imp\n",
    "# import os\n",
    "# import subprocess\n",
    "# import sys\n",
    "# from datetime import date, datetime, timedelta\n",
    "# from importlib import reload\n",
    "# from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "if \"JAVA_HOME\" not in os.environ:\n",
    "    os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "if \"SPARK_HOME\" not in os.environ:\n",
    "    os.environ[\"SPARK_HOME\"] = \"/data/DSTeam/DScommon/spark-3.2.3-bin-hadoop2.7\"\n",
    "# if 'PYSPARK_PYTHON' not in os.environ:\n",
    "#     os.environ['PYSPARK_PYTHON'] = \"/data/DSTeam/DScommon/anaconda3/envs/py38DS/bin/python\"\n",
    "# if 'PYSPARK_DRIVER_PYTHON' not in os.environ:\n",
    "#     os.environ['PYSPARK_DRIVER_PYTHON'] = \"ipython\"\n",
    "\n",
    "sys.path.append(os.environ[\"SPARK_HOME\"] + \"/python/lib/pyspark.zip\")\n",
    "sys.path.append(os.environ[\"SPARK_HOME\"] + \"/python/lib/py4j-0.10.9.5-src.zip\")\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "os.environ['HADOOP_USER_NAME']='detrainer'\n",
    "\n",
    "spark = SparkSession.builder.appName('test_pyspark_thongdt')\\\n",
    "    .config(\"spark.master\", 'local[8]')\\\n",
    "    .config(\"spark.cores.max\", '16')\\\n",
    "    .config(\"spark.driver.memory\", '16g')\\\n",
    "    .config(\"spark.executor.memory\", '16gb')\\\n",
    "    .config(\"spark.sql.caseSensitive\", \"false\")\\\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", \"true\")\\\n",
    "    .config(\"spark.sql.repl.eagerEval.maxNumRows\", \"5\")\\\n",
    "    .config(\"spark.blockManager.port\", '40000')\\\n",
    "    .config(\"spark.driver.port\", '40000')\\\n",
    "    .config(\"spark.port.maxRetries\", '200')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/07 09:49:25 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.parquet('/data/enriched/vas-txn/2020-12-07')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>APP_MOBILE_HASH</th><th>IS_PROMOTED</th><th>SUM_AMOUNT</th><th>SUM_PROMOTIONVALUE</th><th>TOTAL_TRANSACTION</th><th>PROMOTIONCODE_FREQUENCY</th></tr>\n",
       "<tr><td>7Z5lah8T99ZKSpJHf...</td><td>1</td><td>1326800.0</td><td>60000.0</td><td>2</td><td>{kidsplazaqr -&gt; 2}</td></tr>\n",
       "<tr><td>ba7Xt3jkbWogRank7...</td><td>0</td><td>5000000.0</td><td>0.0</td><td>1</td><td>null</td></tr>\n",
       "<tr><td>rS2lahgriTyAwo9lE...</td><td>1</td><td>19000.0</td><td>20000.0</td><td>1</td><td>{vnpaygs25 -&gt; 1}</td></tr>\n",
       "<tr><td>tCHC9WbcnrbRHY6nJ...</td><td>1</td><td>19000.0</td><td>20000.0</td><td>1</td><td>{vnpayguta -&gt; 1}</td></tr>\n",
       "<tr><td>6sdtD9u8fCmz+Hw11...</td><td>1</td><td>602500.0</td><td>52000.0</td><td>2</td><td>{kidsplazaqr -&gt; 1...</td></tr>\n",
       "</table>\n",
       "only showing top 5 rows\n"
      ],
      "text/plain": [
       "+--------------------+-----------+----------+------------------+-----------------+-----------------------+\n",
       "|     APP_MOBILE_HASH|IS_PROMOTED|SUM_AMOUNT|SUM_PROMOTIONVALUE|TOTAL_TRANSACTION|PROMOTIONCODE_FREQUENCY|\n",
       "+--------------------+-----------+----------+------------------+-----------------+-----------------------+\n",
       "|7Z5lah8T99ZKSpJHf...|          1| 1326800.0|           60000.0|                2|     {kidsplazaqr -> 2}|\n",
       "|ba7Xt3jkbWogRank7...|          0| 5000000.0|               0.0|                1|                   null|\n",
       "|rS2lahgriTyAwo9lE...|          1|   19000.0|           20000.0|                1|       {vnpaygs25 -> 1}|\n",
       "|tCHC9WbcnrbRHY6nJ...|          1|   19000.0|           20000.0|                1|       {vnpayguta -> 1}|\n",
       "|6sdtD9u8fCmz+Hw11...|          1|  602500.0|           52000.0|                2|   {kidsplazaqr -> 1...|\n",
       "+--------------------+-----------+----------+------------------+-----------------+-----------------------+\n",
       "only showing top 5 rows"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import  reload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'sfivn.video_preprocessor.video_handler' from '/opt/code/street_food_vn/notebooks/../src/sfivn/video_preprocessor/video_handler.py'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(video_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sfivn.video_preprocessor import video_handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: Couldn't read video stream from file \"../data/data_video/Gangnam Style.mp4\"\n",
      "\u001b[32m2023-09-29 13:24:02.743\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msfivn.video_preprocessor.video_handler\u001b[0m:\u001b[36msplit_video_to_images\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mDone open video\u001b[0m\n",
      "\u001b[32m2023-09-29 13:24:02.743\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msfivn.video_preprocessor.video_handler\u001b[0m:\u001b[36msplit_video_to_images\u001b[0m:\u001b[36m62\u001b[0m - \u001b[1mstart extract frames\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "video_handler.framing_video_base_on_video_id(\n",
    "    id='9bZkp7q19f0',\n",
    "    frames_output_dir='../data/data'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad02d7e620237221ded942cfaaeff3f220688a1364709eb89e87f70df9b70a0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
